import streamlit as st
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import altair as alt
import json
import string
import re
import unicodedata
from functools import lru_cache
from collections import Counter

# online-–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
try:
    from deep_translator import GoogleTranslator
    _gt = GoogleTranslator(source="auto", target="en")
except Exception:
    _gt = None  # –Ω–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏/–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç

st.set_page_config(layout="wide", page_title="40 week courses")

SPREADSHEET_ID = "1fR8_Ay7jpzmPCAl6dWSCC7sWw5VJOaNpu5Zp8b78LRg"

# ---- –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ st.secrets (—Å—Ç—Ä–æ–∫–∞ JSON) ----
scope = ["https://spreadsheets.google.com/feeds",
         "https://www.googleapis.com/auth/drive"]
sa_info = json.loads(st.secrets["GCP_SERVICE_ACCOUNT"])
creds = ServiceAccountCredentials.from_json_keyfile_dict(sa_info, scope)
client = gspread.authorize(creds)

# ==================== –£–¢–ò–õ–ò–¢–´ ====================

def load_sheet_as_letter_df(sheet_name: str) -> pd.DataFrame:
    ws = client.open_by_key(SPREADSHEET_ID).worksheet(sheet_name)
    values = ws.get_all_values()
    if not values or len(values) < 2:
        return pd.DataFrame()
    num_cols = len(values[0])
    letters = list(string.ascii_uppercase)[:num_cols]  # A..Z
    return pd.DataFrame(values[1:], columns=letters)

def safe_minmax(dt1_min, dt1_max, dt2_min, dt2_max):
    mins = [d for d in [dt1_min, dt2_min] if pd.notna(d)]
    maxs = [d for d in [dt1_max, dt2_max] if pd.notna(d)]
    return (min(mins) if mins else pd.NaT, max(maxs) if maxs else pd.NaT)

def filter_df(df: pd.DataFrame, course_col: str, date_col: str,
              selected_courses, date_range):
    if df.empty:
        return df
    dff = df.copy()
    if selected_courses:
        dff = dff[dff[course_col].isin(selected_courses)]
    else:
        return dff.iloc[0:0]
    if isinstance(date_range, (list, tuple)) and len(date_range) == 2:
        start_dt = pd.to_datetime(date_range[0])
        end_dt = pd.to_datetime(date_range[1])
        dff = dff[(dff[date_col] >= start_dt) & (dff[date_col] <= end_dt)]
    elif isinstance(date_range, (list, tuple)) and len(date_range) == 1:
        only_dt = pd.to_datetime(date_range[0])
        dff = dff[dff[date_col].dt.date == only_dt.date()]
    return dff

def apply_filters_and_aggregate(df: pd.DataFrame, course_col: str, date_col: str,
                                x_col: str, y_col: str,
                                selected_courses=None, date_range=None):
    dff = filter_df(df, course_col, date_col, selected_courses, date_range)
    grp = dff.dropna(subset=[x_col, y_col])
    if grp.empty:
        return pd.DataFrame(columns=[x_col, "avg_y", "count"])
    agg = (grp.groupby(x_col, as_index=False)
             .agg(avg_y=(y_col, "mean"),
                  count=(y_col, "size"))
             .sort_values(x_col))
    return agg

def add_bucket(dff: pd.DataFrame, date_col: str, granularity: str) -> pd.DataFrame:
    out = dff.copy()
    if out.empty:
        return out
    if granularity == "–î–µ–Ω—å":
        out["bucket"] = out[date_col].dt.floor("D")
    elif granularity == "–ù–µ–¥–µ–ª—è":
        out["bucket"] = out[date_col].dt.to_period("W-MON").dt.start_time
    elif granularity == "–ú–µ—Å—è—Ü":
        out["bucket"] = out[date_col].dt.to_period("M").dt.to_timestamp()
    elif granularity == "–ì–æ–¥":
        out["bucket"] = out[date_col].dt.to_period("Y").dt.to_timestamp()
    else:
        out["bucket"] = out[date_col].dt.floor("D")
    return out

def ensure_bucket_and_label(dff: pd.DataFrame, date_col: str, granularity: str) -> pd.DataFrame:
    if dff.empty:
        return dff.copy()
    out = dff.copy()
    if "bucket" not in out.columns or not pd.api.types.is_datetime64_any_dtype(out["bucket"]):
        out = add_bucket(out, date_col, granularity)

    if granularity == "–î–µ–Ω—å":
        fmt = "%Y-%m-%d"
    elif granularity == "–ù–µ–¥–µ–ª—è":
        fmt = "W%W (%Y-%m-%d)"
    elif granularity == "–ú–µ—Å—è—Ü":
        fmt = "%Y-%m"
    elif granularity == "–ì–æ–¥":
        fmt = "%Y"
    else:
        fmt = "%Y-%m-%d"

    out["bucket_label"] = out["bucket"].dt.strftime(fmt)
    return out

def prep_distribution(df_f: pd.DataFrame, value_col: str, allowed_values: list, label_title: str):
    if df_f.empty:
        return pd.DataFrame(), [], [], label_title
    d = df_f[df_f[value_col].isin(allowed_values)].copy()
    if d.empty:
        return pd.DataFrame(), [], [], label_title

    d["val"] = d[value_col].astype(int)
    d["val_str"] = d["val"].astype(str)

    grp = (d.groupby(["bucket", "bucket_label", "val", "val_str"], as_index=False)
             .size()
             .rename(columns={"size": "count"}))

    totals = (grp.groupby(["bucket", "bucket_label"], as_index=False)["count"]
                .sum()
                .rename(columns={"count": "total"}))

    out = grp.merge(totals, on=["bucket", "bucket_label"], how="left")
    out["pct"] = out["count"] / out["total"]

    bucket_order = (out[["bucket", "bucket_label"]]
                    .drop_duplicates()
                    .sort_values("bucket")["bucket_label"].tolist())

    val_order = [str(v) for v in allowed_values]
    return out, bucket_order, val_order, label_title

def prep_distribution_text_fr2(df_f: pd.DataFrame, text_col: str, granularity: str, title: str):
    """
    –ì–æ—Ç–æ–≤–∏—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ (FR2),
    –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ EN, —Å—á–∏—Ç–∞–µ—Ç count –∏ % –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (out_df, bucket_order, cat_order, title)
    –≥–¥–µ out_df –∏–º–µ–µ—Ç –∫–æ–ª–æ–Ω–∫–∏: bucket, bucket_label, cat_en, count, total, pct
    """
    if df_f.empty or text_col not in df_f.columns:
        return pd.DataFrame(), [], [], title

    d = df_f.copy()
    d = d.dropna(subset=["A", text_col])

    # –ø—Ä–∏–≤–æ–¥–∏–º –∫ –ø–µ—Ä–∏–æ–¥–∞–º
    d = add_bucket(d, "A", granularity)
    d = ensure_bucket_and_label(d, "A", granularity)

    rows = []
    splitter = re.compile(r"[;,/\n|]+")
    for _, r in d.iterrows():
        raw = str(r[text_col]).strip()
        if not raw:
            continue
        parts = splitter.split(raw) if splitter.search(raw) else [raw]
        for p in parts:
            val = p.strip()
            if not val:
                continue
            en = translate_es_to_en(val)
            rows.append((r["bucket"], r["bucket_label"], en))

    if not rows:
        return pd.DataFrame(), [], [], title

    out = (pd.DataFrame(rows, columns=["bucket","bucket_label","cat_en"])
             .groupby(["bucket","bucket_label","cat_en"], as_index=False)
             .size()
             .rename(columns={"size":"count"}))

    totals = (out.groupby(["bucket","bucket_label"], as_index=False)["count"]
                .sum().rename(columns={"count":"total"}))
    out = out.merge(totals, on=["bucket","bucket_label"], how="left")
    out["pct"] = out["count"] / out["total"]

    bucket_order = (out[["bucket","bucket_label"]]
                    .drop_duplicates()
                    .sort_values("bucket")["bucket_label"].tolist())

    # –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—É–º–º–µ –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
    cat_order = (out.groupby("cat_en", as_index=False)["count"]
                   .sum()
                   .sort_values("count", ascending=False)["cat_en"]
                   .tolist())

    return out, bucket_order, cat_order, title

# ===== –ê—Å–ø–µ–∫—Ç—ã –∏ –ø–µ—Ä–µ–≤–æ–¥ =====
ASPECTS_ES_EN = [
    ("La materia que se ense√±√≥", "The subject that was taught"),
    ("La explicaci√≥n del profesor", "The teacher's explanation"),
    ("Actividades realizadas en la sala", "Activities done in the classroom"),
    ("Tareas para hacer en casa", "Homework to do at home"),
    ("La forma en que se comport√≥ la clase", "How the class behaved"),
    ("Cuando me aclararon las dudas", "When my questions were clarified"),
    ("Cuando me trataron bien y con atenci√≥n", "When I was treated well and attentively"),
]

def _norm(s: str) -> str:
    if not isinstance(s, str):
        s = "" if pd.isna(s) else str(s)
    s = unicodedata.normalize("NFKD", s).casefold().strip()
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

_ASPECTS_NORM = [(_norm(es), es, en) for es, en in ASPECTS_ES_EN]

_BASIC_ES_EN = {
    "si": "yes", "s√≠": "yes", "no": "no", "ok": "ok",
    "materia":"subject","explicacion":"explanation","explicaci√≥n":"explanation",
    "profesor":"teacher","actividades":"activities","sala":"classroom",
    "tareas":"homework","casa":"home","forma":"way","comporto":"behaved",
    "comport√≥":"behaved","clase":"class","aclararon":"clarified","dudas":"doubts",
    "trataron":"treated","bien":"well","atencion":"attention","atenci√≥n":"attention"
}

def _naive_translate_es_en(text: str) -> str:
    tokens = re.findall(r"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]+", text)
    mapped = []
    for t in tokens:
        k = unicodedata.normalize("NFKD", t).encode("ascii", "ignore").decode("ascii").lower()
        mapped.append(_BASIC_ES_EN.get(k, t))
    out = " ".join(mapped).strip()
    return out[0].upper() + out[1:] if out else ""

@lru_cache(maxsize=5000)
def translate_es_to_en(text: str) -> str:
    t = (text or "").strip()
    if not t:
        return ""
    if _gt is not None:
        try:
            return _gt.translate(t)
        except Exception:
            pass
    return _naive_translate_es_en(t)

# ===== Dislike-–∞—Å–ø–µ–∫—Ç—ã (–∏–∑ F), —Å–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º EN-–ø–µ—Ä–µ–≤–æ–¥–æ–º =====
DISLIKE_ES_EN = [
    ("Explica mejor el contenido.", "Explain the content better."),
    ("Tener menos retrasos o ausencias.", "Have fewer delays or absences."),
    ("Hacer m√°s preguntas.", "Ask more questions."),
    ("Prestar m√°s atenci√≥n a los estudiantes.", "Pay more attention to students."),
    ("Mejorar la disciplina de la clase.", "Improve class discipline."),
    ("Responde m√°s en WhatsApp.", "Reply more on WhatsApp."),
]

def _norm_local(s: str) -> str:
    if not isinstance(s, str):
        s = "" if pd.isna(s) else str(s)
    s = unicodedata.normalize("NFKD", s).casefold().strip()
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s

# —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∞—Å–ø–µ–∫—Ç–æ–≤ [(es, en), ...]
def build_aspects_counts_generic(df: pd.DataFrame, text_col: str, date_col: str,
                                 granularity: str, aspects_es_en: list[tuple[str, str]]):
    need_cols = [date_col, text_col]
    if df.empty or not all(c in df.columns for c in need_cols):
        return (pd.DataFrame(columns=["bucket","bucket_label","aspect","aspect_en","count"]),
                pd.DataFrame(columns=["en","mention","total"]))

    d = df[need_cols].copy()
    d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
    d = d.dropna(subset=[date_col])
    if d.empty:
        return (pd.DataFrame(columns=["bucket","bucket_label","aspect","aspect_en","count"]),
                pd.DataFrame(columns=["en","mention","total"]))

    aspects_norm = [(_norm_local(es), es, en) for es, en in aspects_es_en]

    d = d.rename(columns={date_col: "A", text_col: "TXT"})
    d = add_bucket(d, "A", granularity)
    d = ensure_bucket_and_label(d, "A", granularity)

    rows, _unknown = [], []
    for _, r in d.iterrows():
        txt = str(r["TXT"]).strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            t = _norm_local(p.strip())
            if not t:
                continue
            for es_norm, es, en in aspects_norm:
                if t == es_norm or es_norm in t:
                    rows.append((r["bucket"], r["bucket_label"], f"{es} (EN: {en})", en, 1))
                    break

    counts = pd.DataFrame(rows, columns=["bucket","bucket_label","aspect","aspect_en","count"])
    if not counts.empty:
        counts = counts.groupby(["bucket","bucket_label","aspect","aspect_en"], as_index=False)["count"].sum()

    return counts, pd.DataFrame(columns=["en","mention","total"])

# ===== FR2: —à–∞–±–ª–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è D –∏ E =====
FR2_D_TEMPL_ES_EN = [
    ("S√≠, todo a tiempo", "Yes, everything on time"),
    ("Empez√≥ tarde la clase", "The class started late"),
    ("El profesor lleg√≥ muy tarde o no vino en absoluto", "The teacher arrived very late or didn't come at all"),
]

FR2_E_TEMPL_ES_EN = [
    ("S√≠", "Yes"),
    ("Termin√≥ demasiado pronto", "Finished too soon"),
    ("Fue demasiado larga", "It was too long"),
    ("No hubo clase", "There was no class"),
]

def build_template_counts(
    df: pd.DataFrame,
    text_col: str,
    date_col: str,
    granularity: str,
    templates_es_en: list[tuple[str, str]],
):
    need_cols = [date_col, text_col]
    if df.empty or not all(c in df.columns for c in need_cols):
        return pd.DataFrame(columns=["bucket","bucket_label","templ_es","templ_en","count"])

    d = df[need_cols].copy()
    d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
    d = d.dropna(subset=[date_col])
    if d.empty:
        return pd.DataFrame(columns=["bucket","bucket_label","templ_es","templ_en","count"])

    tmpl_norm = [(_norm_local(es), es, en) for es, en in templates_es_en]

    d = d.rename(columns={date_col: "A", text_col: "TXT"})
    d = add_bucket(d, "A", granularity)
    d = ensure_bucket_and_label(d, "A", granularity)

    rows = []
    # ‚ö†Ô∏è –∑–∞–ø—è—Ç—É—é –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å "S√≠, todo a tiempo"
    splitter = re.compile(r"[;\/\n|]+")  # ; / | –∏ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    for _, r in d.iterrows():
        raw = str(r["TXT"] or "").strip()
        if not raw:
            continue
        parts = splitter.split(raw) if splitter.search(raw) else [raw]
        for p in parts:
            t = _norm_local(p.strip())
            if not t:
                continue
            for es_norm, es, en in tmpl_norm:
                if t == es_norm or es_norm in t:
                    rows.append((r["bucket"], r["bucket_label"], es, en))
                    break

    if not rows:
        return pd.DataFrame(columns=["bucket","bucket_label","templ_es","templ_en","count"])

    out = (pd.DataFrame(rows, columns=["bucket","bucket_label","templ_es","templ_en"])
             .groupby(["bucket","bucket_label","templ_es","templ_en"], as_index=False)
             .size().rename(columns={"size":"count"}))
    return out

def build_aspects_counts(df: pd.DataFrame, text_col: str, date_col: str, granularity: str):
    need_cols = [date_col, text_col]
    if df.empty or not all(c in df.columns for c in need_cols):
        return (pd.DataFrame(columns=["bucket","bucket_label","aspect","count"]),
                pd.DataFrame(columns=["en","mention","total"]))

    d = df[need_cols].copy()
    d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
    d = d.dropna(subset=[date_col])
    if d.empty:
        return (pd.DataFrame(columns=["bucket","bucket_label","aspect","count"]),
                pd.DataFrame(columns=["en","mention","total"]))

    d = d.rename(columns={date_col: "A", text_col: "TXT"})
    d = add_bucket(d, "A", granularity)
    d = ensure_bucket_and_label(d, "A", granularity)

    rows, unknown = [], []
    for _, r in d.iterrows():
        txt = str(r["TXT"]).strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            t = _norm(p.strip())
            if not t:
                continue
            matched = False
            for es_norm, es, en in _ASPECTS_NORM:
                if t == es_norm or es_norm in t:
                    rows.append((r["bucket"], r["bucket_label"], f"{es} (EN: {en})", 1))
                    matched = True
                    break
            if not matched:
                unknown.append(p.strip())

    counts = pd.DataFrame(rows, columns=["bucket","bucket_label","aspect","count"])
    if not counts.empty:
        counts = counts.groupby(["bucket","bucket_label","aspect"], as_index=False)["count"].sum()

    unknown_df = pd.DataFrame(unknown, columns=["mention"])
    if not unknown_df.empty:
        unknown_df["mention"] = unknown_df["mention"].str.strip()
        unknown_df = (unknown_df.groupby("mention", as_index=False)
                      .size().rename(columns={"size": "total"})
                      .sort_values("total", ascending=False))
        unknown_df["en"] = unknown_df["mention"].apply(translate_es_to_en)
        unknown_df = unknown_df[["en", "mention", "total"]]
    else:
        unknown_df = pd.DataFrame(columns=["en", "mention", "total"])

    return counts, unknown_df

def build_aspects_counts_by_S(df: pd.DataFrame) -> pd.DataFrame:
    """–°—á—ë—Ç—á–∏–∫–∏ –∞—Å–ø–µ–∫—Ç–æ–≤ –ø–æ —É—Ä–æ–∫–∞–º S (—Ç–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã, EN)."""
    if df.empty or not {"S","E"}.issubset(df.columns):
        return pd.DataFrame(columns=["S","aspect_en","count"])
    d = df[["S","E"]].copy()
    d["S"] = pd.to_numeric(d["S"], errors="coerce")
    d = d.dropna(subset=["S"])
    d["S"] = d["S"].astype(int)

    rows = []
    for _, r in d.iterrows():
        txt = str(r["E"] or "").strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            t = _norm(p.strip())
            if not t:
                continue
            for es_norm, es, en in _ASPECTS_NORM:
                if t == es_norm or es_norm in t:
                    rows.append((int(r["S"]), en, 1))
                    break

    if not rows:
        return pd.DataFrame(columns=["S","aspect_en","count"])
    out = pd.DataFrame(rows, columns=["S","aspect_en","count"])
    out = out.groupby(["S","aspect_en"], as_index=False)["count"].sum()
    return out

def aspect_to_en_label(s: str) -> str:
    """–ò–∑ 'ES (EN: EN)' –¥–æ—Å—Ç–∞—ë–º 'EN'."""
    m = re.search(r"\(EN:\s*(.*?)\)\s*$", str(s))
    return m.group(1).strip() if m else str(s)

def build_dislike_counts_by_S(df: pd.DataFrame) -> pd.DataFrame:
    """–°—á—ë—Ç—á–∏–∫–∏ dislike-–∞—Å–ø–µ–∫—Ç–æ–≤ –ø–æ —É—Ä–æ–∫–∞–º S –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ F (FR1)."""
    if df.empty or not {"S", "F"}.issubset(df.columns):
        return pd.DataFrame(columns=["S", "aspect_en", "count"])

    d = df[["S", "F"]].copy()
    d["S"] = pd.to_numeric(d["S"], errors="coerce")
    d = d.dropna(subset=["S"])
    d["S"] = d["S"].astype(int)

    dislike_norm = [(_norm_local(es), en) for es, en in DISLIKE_ES_EN]

    rows = []
    for _, r in d.iterrows():
        txt = str(r["F"] or "").strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            t = _norm_local(p.strip())
            if not t:
                continue
            for es_norm, en in dislike_norm:
                if t == es_norm or es_norm in t:
                    rows.append((int(r["S"]), en, 1))
                    break

    if not rows:
        return pd.DataFrame(columns=["S", "aspect_en", "count"])

    out = pd.DataFrame(rows, columns=["S", "aspect_en", "count"])
    out = out.groupby(["S", "aspect_en"], as_index=False)["count"].sum()
    return out

def build_template_counts_by_R(
    df: pd.DataFrame,
    text_col: str,
    templates_es_en: list[tuple[str, str]],
) -> pd.DataFrame:
    """
    –°—á—ë—Ç—á–∏–∫–∏ —à–∞–±–ª–æ–Ω–æ–≤ –ø–æ —É—Ä–æ–∫–∞–º R (FR2).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame [R(int), templ_en(str), count(int)].
    –í–∞–∂–Ω–æ: –∑–∞–ø—è—Ç–∞—è –ù–ï —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º (—á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å 'S√≠, todo a tiempo').
    """
    if df.empty or not {"R", text_col}.issubset(df.columns):
        return pd.DataFrame(columns=["R", "templ_en", "count"])

    d = df[["R", text_col]].copy()
    d["R"] = pd.to_numeric(d["R"], errors="coerce")
    d = d.dropna(subset=["R"])
    d["R"] = d["R"].astype(int)

    # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
    tmpl_norm = [(_norm_local(es), en) for es, en in templates_es_en]

    rows = []
    splitter = re.compile(r"[;\/\n|]+")  # ; / | –∏ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ (–ë–ï–ó –∑–∞–ø—è—Ç–æ–π)
    for _, r in d.iterrows():
        raw = str(r[text_col] or "").strip()
        if not raw:
            continue
        parts = splitter.split(raw) if splitter.search(raw) else [raw]
        for p in parts:
            t = _norm_local(p.strip())
            if not t:
                continue
            for es_norm, en in tmpl_norm:
                if t == es_norm or es_norm in t:
                    rows.append((int(r["R"]), en, 1))
                    break

    if not rows:
        return pd.DataFrame(columns=["R", "templ_en", "count"])

    out = (pd.DataFrame(rows, columns=["R", "templ_en", "count"])
           .groupby(["R", "templ_en"], as_index=False)["count"].sum())
    return out


# ==================== –î–ê–ù–ù–´–ï ====================

df1 = load_sheet_as_letter_df("Form Responses 1")   # A=date, N=course, S=x, G=y, E=aspects, F=dislike, H=comments
df2 = load_sheet_as_letter_df("Form Responses 2")   # A=date, M=course, R=x, I=y

# –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
for df, date_col, x_col, y_col in [
    (df1, "A", "S", "G"),
    (df2, "A", "R", "I"),
]:
    if df.empty:
        continue
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df[x_col] = pd.to_numeric(df[x_col], errors="coerce")
    df[y_col] = pd.to_numeric(df[y_col], errors="coerce")

# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: FR2 ‚Äî –ø—Ä–∏–≤–µ—Å—Ç–∏ F, G, H –∫ —á–∏—Å–ª—É (–µ—Å–ª–∏ –æ–Ω–∏ —à–∫–∞–ª—å–Ω—ã–µ)
if not df2.empty:
    for col in ["F", "G", "H"]:
        if col in df2.columns:
            df2[col] = pd.to_numeric(df2[col], errors="coerce")

# ==================== –ï–î–ò–ù–´–ï –§–ò–õ–¨–¢–†–´ ====================

st.sidebar.header("–§–∏–ª—å—Ç—Ä—ã")

# –ö—É—Ä—Å—ã
courses_union = sorted(list(set(
    ([] if df1.empty else df1["N"].dropna().unique().tolist()) +
    ([] if df2.empty else df2["M"].dropna().unique().tolist())
)))
if "courses_selected" not in st.session_state:
    st.session_state["courses_selected"] = courses_union.copy()

b1, b2 = st.sidebar.columns(2)
if b1.button("Select all"):
    st.session_state["courses_selected"] = courses_union.copy()
    st.rerun()
if b2.button("Clear"):
    st.session_state["courses_selected"] = []
    st.rerun()

selected_courses = st.sidebar.multiselect(
    "–ö—É—Ä—Å—ã",
    options=courses_union,
    default=st.session_state["courses_selected"],
    key="courses_selected",
    help="–ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ; –ø–æ–∏—Å–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è."
)
st.sidebar.caption(f"–í—ã–±—Ä–∞–Ω–æ: {len(selected_courses)} –∏–∑ {len(courses_union)}")

# –î–∞—Ç–∞
min1, max1 = (df1["A"].min(), df1["A"].max()) if not df1.empty else (pd.NaT, pd.NaT)
min2, max2 = (df2["A"].min(), df2["A"].max()) if not df2.empty else (pd.NaT, pd.NaT)
glob_min, glob_max = safe_minmax(min1, max1, min2, max2)
if pd.isna(glob_min) or pd.isna(glob_max):
    date_range = st.sidebar.date_input("–î–∞—Ç–∞ —Ñ–∏–¥–±–µ–∫–∞ (A)", [])
else:
    date_range = st.sidebar.date_input("–î–∞—Ç–∞ —Ñ–∏–¥–±–µ–∫–∞ (A)", [glob_min.date(), glob_max.date()])

# –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å
granularity = st.sidebar.selectbox("–ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", ["–î–µ–Ω—å", "–ù–µ–¥–µ–ª—è", "–ú–µ—Å—è—Ü", "–ì–æ–¥"])
BAR_SIZE = {"–î–µ–Ω—å": 18, "–ù–µ–¥–µ–ª—è": 44, "–ú–µ—Å—è—Ü": 56, "–ì–æ–¥": 64}
bar_size = BAR_SIZE.get(granularity, 36)

# ---- –ë–ê–ó–û–í–´–ï –§–ò–õ–¨–¢–†–´ –ü–û –ö–£–†–°–ê–ú/–î–ê–¢–ï –î–õ–Ø –û–ë–û–ò–• –õ–ò–°–¢–û–í ----
df1_base = filter_df(df1, "N", "A", selected_courses, date_range)
df2_base = filter_df(df2, "M", "A", selected_courses, date_range)

# ---- –ï–î–ò–ù–´–ô –§–ò–õ–¨–¢–† –£–†–û–ö–û–í S/R (union –∑–Ω–∞—á–µ–Ω–∏–π S –∏–∑ FR1 –∏ R –∏–∑ FR2) ----
if not df1_base.empty and "S" in df1_base.columns:
    s_vals = pd.to_numeric(df1_base["S"], errors="coerce").dropna().astype(int).unique().tolist()
else:
    s_vals = []
if not df2_base.empty and "R" in df2_base.columns:
    r_vals = pd.to_numeric(df2_base["R"], errors="coerce").dropna().astype(int).unique().tolist()
else:
    r_vals = []
lessons_options = sorted(list(set(s_vals + r_vals)))

if "s_selected" not in st.session_state:
    st.session_state["s_selected"] = lessons_options.copy()

sb1, sb2 = st.sidebar.columns(2)
if sb1.button("All S/R"):
    st.session_state["s_selected"] = lessons_options.copy()
    st.rerun()
if sb2.button("Clear S/R"):
    st.session_state["s_selected"] = []
    st.rerun()

selected_lessons = st.sidebar.multiselect(
    "–£—Ä–æ–∫–∏ (S/R)",
    options=lessons_options,
    default=st.session_state["s_selected"],
    key="s_selected",
    help="–§–∏–ª—å—Ç—Ä –µ–¥–∏–Ω—ã–π –¥–ª—è S (FR1) –∏ R (FR2)"
)

# ==================== –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –§–ò–õ–¨–¢–†–û–í ====================

# –í–µ—Ä—Ö–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏ (—Å—Ä–µ–¥–Ω–∏–µ)
agg1 = apply_filters_and_aggregate(df1, "N", "A", "S", "G", selected_courses, date_range)
if not agg1.empty and selected_lessons:
    agg1["S"] = pd.to_numeric(agg1["S"], errors="coerce").astype("Int64")
    agg1 = agg1[agg1["S"].isin(selected_lessons)]
    agg1["S"] = agg1["S"].astype(int)

agg2 = apply_filters_and_aggregate(df2, "M", "A", "R", "I", selected_courses, date_range)
if not agg2.empty and selected_lessons:
    agg2["R"] = pd.to_numeric(agg2["R"], errors="coerce").astype("Int64")
    agg2 = agg2[agg2["R"].isin(selected_lessons)]
    agg2["R"] = agg2["R"].astype(int)

# –ù–∏–∂–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (—Å—ã—Ä—ã–µ) + S/R-—Ñ–∏–ª—å—Ç—Ä
df1_f = df1_base.copy()
if not df1_f.empty and selected_lessons:
    df1_f["S_num"] = pd.to_numeric(df1_f["S"], errors="coerce")
    df1_f = df1_f[df1_f["S_num"].isin(selected_lessons)]

df2_f = df2_base.copy()
if not df2_f.empty and selected_lessons:
    df2_f["R_num"] = pd.to_numeric(df2_f["R"], errors="coerce")
    df2_f = df2_f[df2_f["R_num"].isin(selected_lessons)]

if not df1_f.empty:
    df1_f = df1_f.dropna(subset=["A", "G"])
    df1_f = add_bucket(df1_f, "A", granularity)
    df1_f = ensure_bucket_and_label(df1_f, "A", granularity)

if not df2_f.empty:
    df2_f = df2_f.dropna(subset=["A", "I"])
    df2_f = add_bucket(df2_f, "A", granularity)
    df2_f = ensure_bucket_and_label(df2_f, "A", granularity)

fr1_out, fr1_bucket_order, fr1_val_order, fr1_title = prep_distribution(df1_f, "G", [1,2,3,4,5], "G")
fr2_out, fr2_bucket_order, fr2_val_order, fr2_title = prep_distribution(df2_f, "I", list(range(1,11)), "I")

# ==================== –û–¢–†–ò–°–û–í–ö–ê ====================

st.title("40 week courses")

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("Form Responses 1 ‚Äî Average by S")
    if agg1.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        y_min = float(agg1["avg_y"].min()) if len(agg1) else 0.0
        y_max = float(agg1["avg_y"].max()) if len(agg1) else 5.0
        pad = (y_max - y_min) * 0.1 if y_max > y_min else 0.5
        y_scale = alt.Scale(domain=[y_min - pad, y_max + pad], nice=False, clamp=True)

        chart1 = (
            alt.Chart(agg1).mark_line(point=True)
              .encode(
                  x=alt.X("S:Q", title="S"),
                  y=alt.Y("avg_y:Q", title="Average G", scale=y_scale),
                  tooltip=[
                      alt.Tooltip("S:Q", title="S"),
                      alt.Tooltip("avg_y:Q", title="Average G", format=".2f"),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
                  ])
              .properties(height=380)
        )
        st.altair_chart(chart1, use_container_width=True, theme=None)

with col2:
    st.subheader("Form Responses 2 ‚Äî Average by R")
    if agg2.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        y_min2 = float(agg2["avg_y"].min()) if len(agg2) else 0.0
        y_max2 = float(agg2["avg_y"].max()) if len(agg2) else 10.0
        pad2 = (y_max2 - y_min2) * 0.1 if y_max2 > y_min2 else 0.5
        y_scale2 = alt.Scale(domain=[y_min2 - pad2, y_max2 + pad2], nice=False, clamp=True)

        chart2 = (
            alt.Chart(agg2).mark_line(point=True)
              .encode(
                  x=alt.X("R:Q", title="R"),
                  y=alt.Y("avg_y:Q", title="Average I", scale=y_scale2),
                  tooltip=[
                      alt.Tooltip("R:Q", title="R"),
                      alt.Tooltip("avg_y:Q", title="Average I", format=".2f"),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
                  ])
              .properties(height=380)
        )
        st.altair_chart(chart2, use_container_width=True, theme=None)

st.markdown("---")
st.subheader(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (–≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å: {granularity.lower()})")

col3, col4 = st.columns([1, 1])

with col3:
    st.markdown("**Form Responses 1 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ G (1‚Äì5)**")
    if fr1_out.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö (FR1).")
    else:
        bars1 = (
            alt.Chart(fr1_out).mark_bar(size=BAR_SIZE.get(granularity, 36))
              .encode(
                  x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=fr1_bucket_order),
                  y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                  color=alt.Color("val_str:N", title=fr1_title, sort=fr1_val_order),
                  order=alt.Order("val:Q", sort="ascending"),
                  tooltip=[
                      alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                      alt.Tooltip("val_str:N", title=fr1_title),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%")
                  ])
              .properties(height=420)
        )
        st.altair_chart(bars1, use_container_width=True, theme=None)

with col4:
    st.markdown("**Form Responses 2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ I (1‚Äì10)**")
    if fr2_out.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö (FR2).")
    else:
        bars2 = (
            alt.Chart(fr2_out).mark_bar(size=BAR_SIZE.get(granularity, 36))
              .encode(
                  x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=fr2_bucket_order),
                  y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                  color=alt.Color("val_str:N", title=fr2_title, sort=fr2_val_order),
                  order=alt.Order("val:Q", sort="ascending"),
                  tooltip=[
                      alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                      alt.Tooltip("val_str:N", title=fr2_title),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%")
                  ])
              .properties(height=420)
        )
        st.altair_chart(bars2, use_container_width=True, theme=None)

# ---------- –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –£–†–û–ö–ê–ú (–≤ %) –î–õ–Ø –¢–ï–• –ñ–ï –®–ö–ê–õ ----------
st.markdown("---")
st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–≤ %) ‚Äî —Ç–µ –∂–µ —à–∫–∞–ª—ã G (FR1) –∏ I (FR2)")

def _build_numeric_counts_by_axis(df_src: pd.DataFrame, axis_col: str, val_col: str, allowed_vals: list[int] | None):
    """
    –ì–æ—Ç–æ–≤–∏—Ç —Å—á—ë—Ç—á–∏–∫–∏ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å = axis_col: 'S' –∏–ª–∏ 'R').
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [axis_col, val, val_str, count, total]
    """
    if df_src.empty or axis_col not in df_src.columns or val_col not in df_src.columns:
        return pd.DataFrame(columns=[axis_col, "val", "val_str", "count", "total"])

    d = df_src.copy()
    d[axis_col] = pd.to_numeric(d[axis_col], errors="coerce")
    d[val_col]  = pd.to_numeric(d[val_col],  errors="coerce")
    d = d.dropna(subset=[axis_col, val_col])
    if d.empty:
        return pd.DataFrame(columns=[axis_col, "val", "val_str", "count", "total"])

    if allowed_vals is not None:
        d = d[d[val_col].isin(allowed_vals)]
        if d.empty:
            return pd.DataFrame(columns=[axis_col, "val", "val_str", "count", "total"])

    d[axis_col] = d[axis_col].astype(int)
    d["val"] = d[val_col].astype(int)
    d["val_str"] = d["val"].astype(str)

    grp = (d.groupby([axis_col, "val", "val_str"], as_index=False)
             .size().rename(columns={"size": "count"}))
    totals = (grp.groupby(axis_col, as_index=False)["count"]
                .sum().rename(columns={"count": "total"}))
    out = grp.merge(totals, on=axis_col, how="left")
    return out

def _make_percent_stack_by_axis(out_df: pd.DataFrame, axis_col: str, legend_title: str):
    """
    –†–∏—Å—É–µ—Ç –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–µ–∫ 0‚Äì100% –ø–æ –æ—Å–∏ axis_col (S/R).
    """
    if out_df.empty:
        return None

    axis_order = sorted(out_df[axis_col].unique().tolist())
    val_order  = sorted(out_df["val"].unique().tolist())
    val_order_str = [str(v) for v in val_order]

    base = alt.Chart(out_df).transform_calculate(pct='datum.count / datum.total')

    chart = (
        base.mark_bar(size=28, stroke=None, strokeWidth=0)
            .encode(
                x=alt.X(f"{axis_col}:O", title=axis_col, sort=axis_order),
                y=alt.Y(
                    "count:Q",
                    stack="normalize",
                    axis=alt.Axis(format="%", title="% –æ—Ç –æ—Ç–≤–µ—Ç–æ–≤"),
                    scale=alt.Scale(domain=[0, 1], nice=False, clamp=True),
                ),
                color=alt.Color(
                    "val_str:N",
                    title=legend_title,
                    sort=val_order_str,  # –ø–æ—Ä—è–¥–æ–∫ –≤ –ª–µ–≥–µ–Ω–¥–µ (–æ—Å—Ç–∞–≤–ª—è–µ–º —á–∏—Å–ª–æ–≤–æ–π)
                    legend=alt.Legend(
                        orient="bottom",
                        direction="horizontal",
                        columns=5,
                        labelLimit=1000,
                        titleLimit=1000,
                        symbolType="square",
                    ),
                ),
                order=alt.Order("val:Q", sort="descending"),  # üëà –∫–ª—é—á–µ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
                tooltip=[
                    alt.Tooltip(f"{axis_col}:O", title=("–£—Ä–æ–∫ (R)" if axis_col=="R" else "–£—Ä–æ–∫ (S)")),
                    alt.Tooltip("val_str:N", title=legend_title),
                    alt.Tooltip("count:Q",  title="–ö–æ–ª-–≤–æ"),
                    alt.Tooltip("pct:Q",    title="–î–æ–ª—è", format=".0%"),
                    alt.Tooltip("total:Q",  title="–í—Å–µ–≥–æ –ø–æ —É—Ä–æ–∫—É"),
                ],
            )
    ).configure_legend(labelLimit=1000, titleLimit=1000)

    return chart

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Å —É—á—ë—Ç–æ–º —Ç–µ–∫—É—â–∏—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫—É—Ä—Å–æ–≤/–¥–∞—Ç/—É—Ä–æ–∫–æ–≤
# –õ–ï–í–´–ô: FR2 (I –ø–æ R)
df2_lessons_I = df2_base.copy()
if not df2_lessons_I.empty and selected_lessons:
    df2_lessons_I["R_num"] = pd.to_numeric(df2_lessons_I["R"], errors="coerce")
    df2_lessons_I = df2_lessons_I[df2_lessons_I["R_num"].isin(selected_lessons)]

# –ü–†–ê–í–´–ô: FR1 (G –ø–æ S)
df1_lessons_G = df1_base.copy()
if not df1_lessons_G.empty and selected_lessons:
    df1_lessons_G["S_num"] = pd.to_numeric(df1_lessons_G["S"], errors="coerce")
    df1_lessons_G = df1_lessons_G[df1_lessons_G["S_num"].isin(selected_lessons)]

left_col, right_col = st.columns(2)

with left_col:
    st.markdown("**FR1 ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (S) ‚Äî G (–≤ %)**")
    out_G_S = _build_numeric_counts_by_axis(df1_lessons_G, axis_col="S", val_col="G", allowed_vals=[1,2,3,4,5])
    if out_G_S.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ G (FR1) –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        ch_G_S = _make_percent_stack_by_axis(out_G_S, axis_col="S", legend_title="G")
        st.altair_chart(ch_G_S.properties(height=460), use_container_width=True, theme=None)

with right_col:
    st.markdown("**FR2 ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (R) ‚Äî I (–≤ %)**")
    out_I_R = _build_numeric_counts_by_axis(df2_lessons_I, axis_col="R", val_col="I", allowed_vals=list(range(1, 11)))
    if out_I_R.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ I (FR2) –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        ch_I_R = _make_percent_stack_by_axis(out_I_R, axis_col="R", legend_title="I")
        st.altair_chart(ch_I_R.properties(height=460), use_container_width=True, theme=None)


# ---------- –ù–ò–ñ–ï: –ê—Å–ø–µ–∫—Ç—ã —É—Ä–æ–∫–∞ ‚Äî Form Responses 1 ----------
st.markdown("---")
st.subheader("–ê—Å–ø–µ–∫—Ç—ã —É—Ä–æ–∫–∞ ‚Äî Form Responses 1")

df_aspects = df1_base.copy()
if not df_aspects.empty and selected_lessons:
    df_aspects["S_num"] = pd.to_numeric(df_aspects["S"], errors="coerce")
    df_aspects = df_aspects[df_aspects["S_num"].isin(selected_lessons)]

asp_counts, _unknown_all = build_aspects_counts(df_aspects, text_col="E", date_col="A", granularity=granularity)

# ---- –ì—Ä–∞—Ñ–∏–∫: ¬´–ê—Å–ø–µ–∫—Ç—ã –ø–æ –¥–∞—Ç–∞–º (–æ—Å—å X ‚Äî A)¬ª —Å –±—ã—Å—Ç—Ä—ã–º –æ–±—â–∏–º —Ç—É–ª—Ç–∏–ø–æ–º ----
st.markdown("**–ê—Å–ø–µ–∫—Ç—ã –ø–æ –¥–∞—Ç–∞–º (–æ—Å—å X ‚Äî A)**")
if asp_counts.empty:
    st.info("–ù–µ –Ω–∞—à—ë–ª —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∞—Å–ø–µ–∫—Ç–æ–≤ (–ª–∏—Å—Ç 'Form Responses 1', –∫–æ–ª–æ–Ω–∫–∞ E).")
else:
    asp_counts["aspect_en"] = asp_counts["aspect"].apply(aspect_to_en_label)

    bucket_order = (asp_counts[["bucket","bucket_label"]]
                    .drop_duplicates()
                    .sort_values("bucket")["bucket_label"].tolist())

    totals_by_bucket = (asp_counts.groupby("bucket_label", as_index=False)["count"]
                                  .sum().rename(columns={"count":"total"}))
    y_max = max(1, int(totals_by_bucket["total"].max()))
    y_scale_bar = alt.Scale(domain=[0, y_max * 1.1], nice=False, clamp=True)

    present = (asp_counts["aspect"].unique().tolist())

    bars = (
        alt.Chart(asp_counts).mark_bar(size=max(40, BAR_SIZE.get(granularity, 36)))
          .encode(
              x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥ (–ø–æ A)", sort=bucket_order),
              y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π", scale=y_scale_bar),
              color=alt.Color("aspect:N", title="–ê—Å–ø–µ–∫—Ç", sort=present)
          )
    )

    # ---------- –±—ã—Å—Ç—Ä—ã–π ¬´–æ–±—â–∏–π —Ç—É–ª—Ç–∏–ø¬ª ----------
    wide = (
        asp_counts
        .pivot_table(index=["bucket","bucket_label"], columns="aspect_en",
                     values="count", aggfunc="sum", fill_value=0)
    )

    col_order = list(wide.sum(axis=0).sort_values(ascending=False).index)

    def _pack_row(r, top_k=6):
        total = int(r[col_order].sum())
        if total == 0:
            return pd.Series({"total": 0, "tooltip_text": ""})
        pairs = [(name, int(r[name])) for name in col_order if r[name] > 0]
        pairs.sort(key=lambda x: x[1], reverse=True)
        pairs = pairs[:top_k]
        lines = [f"{name} ‚Äî {c} ({c/total:.0%})" for name, c in pairs]
        return pd.Series({"total": total, "tooltip_text": "\n".join(lines)})

    packed = wide.apply(_pack_row, axis=1).reset_index()

    bubble = (
        alt.Chart(packed)
          .mark_bar(size=max(40, BAR_SIZE.get(granularity, 36)), opacity=0.001)
          .encode(
              x=alt.X("bucket_label:N", sort=bucket_order),
              y=alt.Y("total:Q", scale=y_scale_bar),
              tooltip=[
                  alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                  alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
                  alt.Tooltip("tooltip_text:N", title=""),
              ]
          )
    )

    st.altair_chart((bars + bubble).properties(height=460),
                    theme=None, use_container_width=True)

# --------- –ì–†–ê–§–ò–ö ¬´–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S)¬ª –í % ---------
st.markdown("---")
st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S) ‚Äî –≥—Ä–∞—Ñ–∏–∫ (–≤ %)")

cnt_by_s_all = build_aspects_counts_by_S(df_aspects)
if cnt_by_s_all.empty:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ —É—Ä–æ–∫–∞–º.")
else:
    base = (
        alt.Chart(cnt_by_s_all)
          .transform_aggregate(count='sum(count)', groupby=['S', 'aspect_en'])
          .transform_joinaggregate(total='sum(count)', groupby=['S'])
          .transform_calculate(pct='datum.count / datum.total')
    )

    legend_domain_en = [en for _, en in ASPECTS_ES_EN]
    
    bars_s = (
        base.mark_bar(size=28, stroke=None, strokeWidth=0)
            .encode(
                x=alt.X("S:O", title="S", sort="ascending"),
                y=alt.Y(
                    "count:Q",
                    stack="normalize",
                    axis=alt.Axis(format="%", title="% –æ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
                    scale=alt.Scale(domain=[0, 1], nice=False, clamp=True)
                ),
                color=alt.Color(
                    "aspect_en:N",
                    title="–ê—Å–ø–µ–∫—Ç (EN)",
                    scale=alt.Scale(domain=legend_domain_en),
                    legend=alt.Legend(
                        orient="bottom",
                        direction="horizontal",
                        columns=2,
                        labelLimit=1000,
                        titleLimit=1000,
                        symbolType="square",
                    ),
                ),
                tooltip=[
                    alt.Tooltip("S:O", title="–£—Ä–æ–∫"),
                    alt.Tooltip("aspect_en:N", title="–ê—Å–ø–µ–∫—Ç"),
                    alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                    alt.Tooltip("pct:Q", title="–î–æ–ª—è", format=".0%"),
                    alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ –ø–æ —É—Ä–æ–∫—É"),
                ],
            )
    ).configure_legend(labelLimit=1000, titleLimit=1000)
    
    st.altair_chart(bars_s.properties(height=460), use_container_width=True, theme=None)

# --------- –¢–ê–ë–õ–ò–¶–ê –í–ù–ò–ó–£ ---------
st.markdown("---")
st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S) ‚Äî —Ç–∞–±–ª–∏—Ü–∞")

rows_aspects = []
unknown_per_s = {}

if not df_aspects.empty and {"S","E"}.issubset(df_aspects.columns):
    df_tmp = df_aspects[["S","E"]].copy()
    df_tmp["S"] = pd.to_numeric(df_tmp["S"], errors="coerce")
    df_tmp = df_tmp.dropna(subset=["S"])
    df_tmp["S"] = df_tmp["S"].astype(int)

    for _, rr in df_tmp.iterrows():
        s_val = int(rr["S"])
        txt = str(rr["E"] or "").strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            p_clean = p.strip()
            t = _norm(p_clean)
            if not t:
                continue
            matched = False
            for es_norm, es, en in _ASPECTS_NORM:
                if t == es_norm or es_norm in t:
                    rows_aspects.append((s_val, en))
                    matched = True
                    break
            if not matched:
                unknown_per_s.setdefault(s_val, Counter())[p_clean] += 1

if not rows_aspects and not unknown_per_s:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
else:
    df_as = (pd.DataFrame(rows_aspects, columns=["S","aspect_en"])
             if rows_aspects else pd.DataFrame(columns=["S","aspect_en"]))
    lesson_rows = []
    lessons_sorted = sorted(set(list(df_as["S"].unique()) + list(unknown_per_s.keys())))

    for s in lessons_sorted:
        if not df_as.empty and s in df_as["S"].values:
            cnt = (df_as[df_as["S"] == s]["aspect_en"]
                   .value_counts()
                   .sort_values(ascending=False))
            total_tpl = int(cnt.sum())
            parts_text = []
            for en_name, c in cnt.items():
                pct = (c / total_tpl) if total_tpl else 0.0
                parts_text.append(f"‚Ä¢ {en_name} ‚Äî {c} ({pct:.0%})")
            aspects_text = "\n".join(parts_text)
        else:
            aspects_text = ""
            total_tpl = 0

        unk_counter = unknown_per_s.get(s, Counter())
        if unk_counter:
            top_items = unk_counter.most_common(10)
            rest = sum(unk_counter.values()) - sum(c for _, c in top_items)
            unk_parts = [f"‚Ä¢ {translate_es_to_en(m)} ({c})" for m, c in top_items]
            if rest > 0:
                unk_parts.append(f"‚Ä¢ ‚Ä¶ (+{rest})")
            unknown_text = "\n".join(unk_parts)
        else:
            unknown_text = ""

        lesson_rows.append({
            "S": int(s),
            "Aspects (EN)": aspects_text,
            "Unknown (EN)": unknown_text,
            "Total (templ.)": total_tpl
        })

    table = pd.DataFrame(lesson_rows).sort_values("S").reset_index(drop=True)
    height = min(800, 100 + 28 * len(table))
    st.dataframe(
        table[["S","Aspects (EN)","Unknown (EN)","Total (templ.)"]],
        use_container_width=True,
        height=height
    )

# –ü–æ–¥—Å–∫–∞–∑–∫–∞, –µ—Å–ª–∏ –æ–Ω–ª–∞–π–Ω-–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
if _gt is None:
    st.caption("‚ö†Ô∏è deep-translator –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–æ—Å–ª–æ–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥.")

# ---------- Dislike dynamics (–ø–æ –¥–∞—Ç–∞–º A, —Ç–µ–∫—Å—Ç –∏–∑ F; –ª–∏—Å—Ç FR1) ----------
st.markdown("---")
st.subheader("Dislike dynamics (–ø–æ –¥–∞—Ç–∞–º A –∏–∑ FR1, —Ç–µ–∫—Å—Ç –∏–∑ F)")

df_dislike_src = filter_df(df1, "N", "A", selected_courses, date_range)
dis_counts, _ = build_aspects_counts_generic(
    df_dislike_src, text_col="F", date_col="A", granularity=granularity,
    aspects_es_en=DISLIKE_ES_EN
)

if dis_counts.empty:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ Dislike dynamics (Form Responses 1, –∫–æ–ª–æ–Ω–∫–∞ F).")
else:
    bucket_order = (dis_counts[["bucket","bucket_label"]]
                    .drop_duplicates().sort_values("bucket")["bucket_label"].tolist())

    expected_labels = [f"{es} (EN: {en})" for es, en in DISLIKE_ES_EN]
    present = [lbl for lbl in expected_labels if lbl in dis_counts["aspect"].unique()]

    bars_dis = (
        alt.Chart(dis_counts)
          .mark_bar(size=max(40, bar_size))
          .encode(
              x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥ (–ø–æ A)", sort=bucket_order),
              y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
              color=alt.Color("aspect:N", title="–ê—Å–ø–µ–∫—Ç", sort=present)
          )
    )

    # ---------- –±—ã—Å—Ç—Ä—ã–π ¬´–æ–±—â–∏–π —Ç—É–ª—Ç–∏–ø¬ª –¥–ª—è dislike ----------
    wide = (
        dis_counts
        .pivot_table(index=["bucket","bucket_label"], columns="aspect_en",
                     values="count", aggfunc="sum", fill_value=0)
    )

    col_order = list(wide.sum(axis=0).sort_values(ascending=False).index)

    def _pack_row_dis(r, top_k=6):
        total = int(r[col_order].sum())
        if total == 0:
            return pd.Series({"total": 0, "tooltip_text": ""})
        pairs = [(name, int(r[name])) for name in col_order if r[name] > 0]
        pairs.sort(key=lambda x: x[1], reverse=True)
        pairs = pairs[:top_k]
        lines = [f"{name} ‚Äî {c} ({c/total:.0%})" for name, c in pairs]
        return pd.Series({"total": total, "tooltip_text": "\n".join(lines)})

    packed_dis = wide.apply(_pack_row_dis, axis=1).reset_index()

    bubble_dis = (
        alt.Chart(packed_dis)
          .mark_bar(size=max(40, bar_size), opacity=0.001)
          .encode(
              x=alt.X("bucket_label:N", sort=bucket_order),
              y=alt.Y("total:Q"),
              tooltip=[
                  alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                  alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
                  alt.Tooltip("tooltip_text:N", title=""),
              ]
          )
    )

    st.altair_chart((bars_dis + bubble_dis).properties(height=460),
                    use_container_width=True, theme=None)

# --------- DISLIKE: ¬´–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S) ‚Äî –≥—Ä–∞—Ñ–∏–∫ (–≤ %)¬ª –∏–∑ F ---------
st.markdown("---")
st.subheader("Dislike –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S) ‚Äî –≥—Ä–∞—Ñ–∏–∫ (–≤ %)")

df_dislike_lessons = df1_base.copy()
if not df_dislike_lessons.empty and selected_lessons:
    df_dislike_lessons["S_num"] = pd.to_numeric(df_dislike_lessons["S"], errors="coerce")
    df_dislike_lessons = df_dislike_lessons[df_dislike_lessons["S_num"].isin(selected_lessons)]

cnt_by_s_dis = build_dislike_counts_by_S(df_dislike_lessons)
if cnt_by_s_dis.empty:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è dislike-–≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ —É—Ä–æ–∫–∞–º.")
else:
    base_dis = (
        alt.Chart(cnt_by_s_dis)
          .transform_aggregate(count='sum(count)', groupby=['S', 'aspect_en'])
          .transform_joinaggregate(total='sum(count)', groupby=['S'])
          .transform_calculate(pct='datum.count / datum.total')
    )

    dislike_domain_en = [en for _, en in DISLIKE_ES_EN]

    bars_s_dis = (
        base_dis.mark_bar(size=28, stroke=None, strokeWidth=0)
            .encode(
                x=alt.X("S:O", title="S", sort="ascending"),
                y=alt.Y(
                    "count:Q",
                    stack="normalize",
                    axis=alt.Axis(format="%", title="% –æ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
                    scale=alt.Scale(domain=[0, 1], nice=False, clamp=True)
                ),
                color=alt.Color(
                    "aspect_en:N",
                    title="Dislike-–∞—Å–ø–µ–∫—Ç (EN)",
                    scale=alt.Scale(domain=dislike_domain_en),
                    legend=alt.Legend(
                        orient="bottom",
                        direction="horizontal",
                        columns=2,
                        labelLimit=1000,
                        titleLimit=1000,
                        symbolType="square",
                    ),
                ),
                tooltip=[
                    alt.Tooltip("S:O", title="–£—Ä–æ–∫"),
                    alt.Tooltip("aspect_en:N", title="Dislike-–∞—Å–ø–µ–∫—Ç"),
                    alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                    alt.Tooltip("pct:Q", title="–î–æ–ª—è", format=".0%"),
                    alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ –ø–æ —É—Ä–æ–∫—É"),
                ],
            )
    ).configure_legend(labelLimit=1000, titleLimit=1000)

    st.altair_chart(bars_s_dis.properties(height=460), use_container_width=True, theme=None)

# --------- DISLIKE: ¬´–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S) ‚Äî —Ç–∞–±–ª–∏—Ü–∞¬ª –∏–∑ F ---------
st.markdown("---")
st.subheader("Dislike –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî S) ‚Äî —Ç–∞–±–ª–∏—Ü–∞")

rows_dislike = []
unknown_dislike_per_s = {}

if not df_dislike_lessons.empty and {"S", "F"}.issubset(df_dislike_lessons.columns):
    df_tmp = df_dislike_lessons[["S", "F"]].copy()
    df_tmp["S"] = pd.to_numeric(df_tmp["S"], errors="coerce")
    df_tmp = df_tmp.dropna(subset=["S"])
    df_tmp["S"] = df_tmp["S"].astype(int)

    dislike_norm = [(_norm_local(es), en) for es, en in DISLIKE_ES_EN]

    for _, rr in df_tmp.iterrows():
        s_val = int(rr["S"])
        txt = str(rr["F"] or "").strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            p_clean = p.strip()
            t = _norm_local(p_clean)
            if not t:
                continue
            matched = False
            for es_norm, en in dislike_norm:
                if t == es_norm or es_norm in t:
                    rows_dislike.append((s_val, en))
                    matched = True
                    break
            if not matched:
                unknown_dislike_per_s.setdefault(s_val, Counter())[p_clean] += 1

if not rows_dislike and not unknown_dislike_per_s:
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã dislike –ø–æ —É—Ä–æ–∫–∞–º.")
else:
    df_dis = (pd.DataFrame(rows_dislike, columns=["S", "aspect_en"])
              if rows_dislike else pd.DataFrame(columns=["S", "aspect_en"]))
    lesson_rows = []
    lessons_sorted = sorted(set(list(df_dis["S"].unique()) + list(unknown_dislike_per_s.keys())))

    for s in lessons_sorted:
        if not df_dis.empty and s in df_dis["S"].values:
            cnt = (df_dis[df_dis["S"] == s]["aspect_en"]
                   .value_counts()
                   .sort_values(ascending=False))
            total_tpl = int(cnt.sum())
            parts_text = []
            for en_name, c in cnt.items():
                pct = (c / total_tpl) if total_tpl else 0.0
                parts_text.append(f"‚Ä¢ {en_name} ‚Äî {c} ({pct:.0%})")
            aspects_text = "\n".join(parts_text)
        else:
            aspects_text = ""
            total_tpl = 0

        unk_counter = unknown_dislike_per_s.get(s, Counter())
        if unk_counter:
            top_items = unk_counter.most_common(10)
            rest = sum(unk_counter.values()) - sum(c for _, c in top_items)
            unk_parts = [f"‚Ä¢ {translate_es_to_en(m)} ({c})" for m, c in top_items]
            if rest > 0:
                unk_parts.append(f"‚Ä¢ ‚Ä¶ (+{rest})")
            unknown_text = "\n".join(unk_parts)
        else:
            unknown_text = ""

        lesson_rows.append({
            "S": int(s),
            "Dislike (EN)": aspects_text,
            "Unknown (EN)": unknown_text,
            "Total (templ.)": total_tpl
        })

    dislike_table = pd.DataFrame(lesson_rows).sort_values("S").reset_index(drop=True)
    height = min(800, 100 + 28 * len(dislike_table))
    st.dataframe(
        dislike_table[["S", "Dislike (EN)", "Unknown (EN)", "Total (templ.)"]],
        use_container_width=True,
        height=height
    )

# --------- ADDITIONAL COMMENTS (–∫–æ–ª–æ–Ω–∫–∞ H, –ø–æ —É—Ä–æ–∫–∞–º S) ---------
st.markdown("---")
st.subheader("Additional comments ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (S)")

df_additional = df1_base.copy()
if not df_additional.empty and selected_lessons:
    df_additional["S_num"] = pd.to_numeric(df_additional["S"], errors="coerce")
    df_additional = df_additional[df_additional["S_num"].isin(selected_lessons)]

if df_additional.empty or not {"S", "H"}.issubset(df_additional.columns):
    st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Additional comments (–æ–∂–∏–¥–∞—é—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∏ S –∏ H –Ω–∞ –ª–∏—Å—Ç–µ Form Responses 1).")
else:
    df_tmp = df_additional[["S", "H"]].copy()
    df_tmp["S"] = pd.to_numeric(df_tmp["S"], errors="coerce")
    df_tmp = df_tmp.dropna(subset=["S"])
    df_tmp["S"] = df_tmp["S"].astype(int)

    comments_per_s = {}
    for _, rr in df_tmp.iterrows():
        s_val = int(rr["S"])
        txt = str(rr["H"] or "").strip()
        if not txt:
            continue
        parts = re.split(r"[;,/\n|]+", txt) if re.search(r"[;,/\n|]", txt) else [txt]
        for p in parts:
            p_clean = p.strip()
            if not p_clean:
                continue
            comments_per_s.setdefault(s_val, Counter())[p_clean] += 1

    if not comments_per_s:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Additional comments –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º.")
    else:
        rows = []
        for s in sorted(comments_per_s.keys()):
            counter = comments_per_s[s]
            total = int(sum(counter.values()))
            items = sorted(counter.items(), key=lambda x: (-x[1], x[0]))
            bullets_en = []
            for txt, c in items:
                pct = (c / total) if total else 0.0
                bullets_en.append(f"‚Ä¢ {translate_es_to_en(txt)} ‚Äî {c} ({pct:.0%})")
            rows.append({
                "S": s,
                "Comments (EN)": "\n".join(bullets_en),
                "Total comments": total,
            })

        add_table = pd.DataFrame(rows).sort_values("S").reset_index(drop=True)
        height = min(900, 120 + 28 * len(add_table))
        st.dataframe(
            add_table[["S", "Comments (EN)", "Total comments"]],
            use_container_width=True,
            height=height
        )

# ---------- FR2: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ —à–∞–±–ª–æ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º D –∏ E ----------
st.markdown("---")
st.subheader(f"Form Responses 2 ‚Äî —à–∞–±–ª–æ–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å: {granularity.lower()})")

# –ò—Å—Ç–æ—á–Ω–∏–∫: df2_base (–∫—É—Ä—Å—ã/–¥–∞—Ç—ã) + —Ñ–∏–ª—å—Ç—Ä –ø–æ —É—Ä–æ–∫–∞–º (R)
def _apply_r_filter(df_src: pd.DataFrame) -> pd.DataFrame:
    if df_src.empty or "R" not in df_src.columns:
        return df_src
    if selected_lessons:
        d = df_src.copy()
        d["R_num"] = pd.to_numeric(d["R"], errors="coerce")
        d = d[d["R_num"].isin(selected_lessons)]
        return d
    return df_src

df2_text_src = _apply_r_filter(df2_base)

colD, colE = st.columns(2)

# ---- D (–ø–æ —à–∞–±–ª–æ–Ω–∞–º) ----
with colD:
    st.markdown("**FR2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ D (—à–∞–±–ª–æ–Ω—ã)**")
    if df2_text_src.empty or "D" not in df2_text_src.columns:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–∫–æ–ª–æ–Ω–∫–∞ D –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –ø—É—Å—Ç—ã–µ).")
    else:
        d_cnt = build_template_counts(df2_text_src, text_col="D", date_col="A",
                                      granularity=granularity, templates_es_en=FR2_D_TEMPL_ES_EN)
        if d_cnt.empty:
            st.info("–ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å —à–∞–±–ª–æ–Ω–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ –¥–ª—è D.")
        else:
            # –ø–µ—Ä–∏–æ–¥ –∏ –ª–µ–≥–µ–Ω–¥–∞
            d_bucket_order = (d_cnt[["bucket","bucket_label"]]
                              .drop_duplicates()
                              .sort_values("bucket")["bucket_label"].tolist())
            legend_domain = [en for _, en in FR2_D_TEMPL_ES_EN]

            # –¥–æ–±–∞–≤–∏–º total/pct –¥–ª—è —Ç—É–ª—Ç–∏–ø–æ–≤
            d_totals = (d_cnt.groupby(["bucket","bucket_label"], as_index=False)["count"]
                            .sum().rename(columns={"count":"total"}))
            d_out = d_cnt.merge(d_totals, on=["bucket","bucket_label"], how="left")
            d_out["pct"] = d_out["count"] / d_out["total"]

            barsD = (
                alt.Chart(d_out)
                   .mark_bar(size=BAR_SIZE.get(granularity, 36))
                   .encode(
                       x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=d_bucket_order),
                       y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                       color=alt.Color(
                           "templ_en:N",
                           title="D (EN)",
                           scale=alt.Scale(domain=legend_domain),
                       ),
                       tooltip=[
                           alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                           alt.Tooltip("templ_en:N", title="–®–∞–±–ª–æ–Ω (EN)"),
                           alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                           alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%"),
                       ],
                   )
                   .properties(height=420)
            )
            st.altair_chart(barsD, use_container_width=True, theme=None)

# ---- E (–ø–æ —à–∞–±–ª–æ–Ω–∞–º) ----
with colE:
    st.markdown("**FR2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ E (—à–∞–±–ª–æ–Ω—ã)**")
    if df2_text_src.empty or "E" not in df2_text_src.columns:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–∫–æ–ª–æ–Ω–∫–∞ E –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –ø—É—Å—Ç—ã–µ).")
    else:
        e_cnt = build_template_counts(df2_text_src, text_col="E", date_col="A",
                                      granularity=granularity, templates_es_en=FR2_E_TEMPL_ES_EN)
        if e_cnt.empty:
            st.info("–ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å —à–∞–±–ª–æ–Ω–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ –¥–ª—è E.")
        else:
            e_bucket_order = (e_cnt[["bucket","bucket_label"]]
                              .drop_duplicates()
                              .sort_values("bucket")["bucket_label"].tolist())
            legend_domain_e = [en for _, en in FR2_E_TEMPL_ES_EN]

            e_totals = (e_cnt.groupby(["bucket","bucket_label"], as_index=False)["count"]
                           .sum().rename(columns={"count":"total"}))
            e_out = e_cnt.merge(e_totals, on=["bucket","bucket_label"], how="left")
            e_out["pct"] = e_out["count"] / e_out["total"]

            barsE = (
                alt.Chart(e_out)
                   .mark_bar(size=BAR_SIZE.get(granularity, 36))
                   .encode(
                       x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=e_bucket_order),
                       y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                       color=alt.Color(
                           "templ_en:N",
                           title="E (EN)",
                           scale=alt.Scale(domain=legend_domain_e),
                       ),
                       tooltip=[
                           alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                           alt.Tooltip("templ_en:N", title="–®–∞–±–ª–æ–Ω (EN)"),
                           alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                           alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%"),
                       ],
                   )
                   .properties(height=420)
            )
            st.altair_chart(barsE, use_container_width=True, theme=None)

# --------- FR2: –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî R) ‚Äî –≥—Ä–∞—Ñ–∏–∫–∏ –≤ % –ø–æ D –∏ E ---------
st.markdown("---")
st.subheader("FR2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî R) ‚Äî –≥—Ä–∞—Ñ–∏–∫–∏ (–≤ %)")

# –ò—Å—Ç–æ—á–Ω–∏–∫ ‚Äî df2_base + —Ñ–∏–ª—å—Ç—Ä –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —É—Ä–æ–∫–∞–º (R)
df2_lessons = df2_base.copy()
if not df2_lessons.empty and selected_lessons:
    df2_lessons["R_num"] = pd.to_numeric(df2_lessons["R"], errors="coerce")
    df2_lessons = df2_lessons[df2_lessons["R_num"].isin(selected_lessons)]

col_left, col_right = st.columns(2)

with col_left:
    st.markdown("**FR2 ‚Äî –ø–æ D (—à–∞–±–ª–æ–Ω—ã), % –≤–Ω—É—Ç—Ä–∏ R**")
    cnt_by_r_D = build_template_counts_by_R(df2_lessons, text_col="D", templates_es_en=FR2_D_TEMPL_ES_EN)
    if cnt_by_r_D.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ D.")
    else:
        legend_domain_D = [en for _, en in FR2_D_TEMPL_ES_EN]
        base_D = (
            alt.Chart(cnt_by_r_D)
              .transform_aggregate(count='sum(count)', groupby=['R', 'templ_en'])
              .transform_joinaggregate(total='sum(count)', groupby=['R'])
              .transform_calculate(pct='datum.count / datum.total')
        )
        bars_D = (
            base_D.mark_bar(size=28, stroke=None, strokeWidth=0)
              .encode(
                  x=alt.X("R:O", title="R", sort="ascending"),
                  y=alt.Y("count:Q",
                          stack="normalize",
                          axis=alt.Axis(format="%", title="% –æ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
                          scale=alt.Scale(domain=[0, 1], nice=False, clamp=True)),
                  color=alt.Color(
                      "templ_en:N",
                      title="Template (EN)",
                      scale=alt.Scale(domain=legend_domain_D),
                      legend=alt.Legend(
                          orient="bottom", direction="horizontal", columns=2,
                          labelLimit=1000, titleLimit=1000, symbolType="square"
                      ),
                  ),
                  tooltip=[
                      alt.Tooltip("R:O", title="–£—Ä–æ–∫"),
                      alt.Tooltip("templ_en:N", title="–®–∞–±–ª–æ–Ω"),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="–î–æ–ª—è", format=".0%"),
                      alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ –ø–æ —É—Ä–æ–∫—É"),
                  ],
              )
        ).configure_legend(labelLimit=1000, titleLimit=1000)
        st.altair_chart(bars_D.properties(height=420), use_container_width=True, theme=None)

with col_right:
    st.markdown("**FR2 ‚Äî –ø–æ E (—à–∞–±–ª–æ–Ω—ã), % –≤–Ω—É—Ç—Ä–∏ R**")
    cnt_by_r_E = build_template_counts_by_R(df2_lessons, text_col="E", templates_es_en=FR2_E_TEMPL_ES_EN)
    if cnt_by_r_E.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ E.")
    else:
        legend_domain_E = [en for _, en in FR2_E_TEMPL_ES_EN]
        base_E = (
            alt.Chart(cnt_by_r_E)
              .transform_aggregate(count='sum(count)', groupby=['R', 'templ_en'])
              .transform_joinaggregate(total='sum(count)', groupby=['R'])
              .transform_calculate(pct='datum.count / datum.total')
        )
        bars_E = (
            base_E.mark_bar(size=28, stroke=None, strokeWidth=0)
              .encode(
                  x=alt.X("R:O", title="R", sort="ascending"),
                  y=alt.Y("count:Q",
                          stack="normalize",
                          axis=alt.Axis(format="%", title="% –æ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"),
                          scale=alt.Scale(domain=[0, 1], nice=False, clamp=True)),
                  color=alt.Color(
                      "templ_en:N",
                      title="Template (EN)",
                      scale=alt.Scale(domain=legend_domain_E),
                      legend=alt.Legend(
                          orient="bottom", direction="horizontal", columns=2,
                          labelLimit=1000, titleLimit=1000, symbolType="square"
                      ),
                  ),
                  tooltip=[
                      alt.Tooltip("R:O", title="–£—Ä–æ–∫"),
                      alt.Tooltip("templ_en:N", title="–®–∞–±–ª–æ–Ω"),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="–î–æ–ª—è", format=".0%"),
                      alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ –ø–æ —É—Ä–æ–∫—É"),
                  ],
              )
        ).configure_legend(labelLimit=1000, titleLimit=1000)
        st.altair_chart(bars_E.properties(height=420), use_container_width=True, theme=None)

# --------- FR2: —Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ "Average by R" –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º F, G, H ---------
st.markdown("---")
st.subheader("Form Responses 2 ‚Äî Averages by R (F, G, H)")

def _avg_by_r(df_src: pd.DataFrame, y_col: str) -> pd.DataFrame:
    """–ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –∫–æ–ª–æ–Ω–∫–µ y_col —Å —É—á—ë—Ç–æ–º –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫—É—Ä—Å–æ–≤/–¥–∞—Ç/—É—Ä–æ–∫–æ–≤ (R)."""
    if df_src.empty or not {"A", "M", "R", y_col}.issubset(df_src.columns):
        return pd.DataFrame(columns=["R", "avg_y", "count"])
    d = df_src.copy()
    # –ø—Ä–∏–º–µ–Ω–∏–º –µ–¥–∏–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä —É—Ä–æ–∫–æ–≤
    if selected_lessons:
        d["R"] = pd.to_numeric(d["R"], errors="coerce")
        d = d[d["R"].isin(selected_lessons)]
    d = d.dropna(subset=["R", y_col])
    if d.empty:
        return pd.DataFrame(columns=["R", "avg_y", "count"])
    out = (d.groupby("R", as_index=False)
             .agg(avg_y=(y_col, "mean"),
                  count=(y_col, "size"))
             .sort_values("R"))
    # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ R –∫ int, —á—Ç–æ–±—ã –æ—Å—å –±—ã–ª–∞ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–π
    out["R"] = out["R"].astype(int)
    return out

aggF = _avg_by_r(df2_base, "F") if "F" in df2.columns else pd.DataFrame()
aggG_2 = _avg_by_r(df2_base, "G") if "G" in df2.columns else pd.DataFrame()
aggH = _avg_by_r(df2_base, "H") if "H" in df2.columns else pd.DataFrame()

colF, colG2, colH = st.columns(3)

def _make_avg_chart(df_avg: pd.DataFrame, title_y: str):
    if df_avg.empty or len(df_avg) == 0:
        return st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    y_min = float(df_avg["avg_y"].min())
    y_max = float(df_avg["avg_y"].max())
    pad = (y_max - y_min) * 0.1 if y_max > y_min else 0.5
    y_scale = alt.Scale(domain=[y_min - pad, y_max + pad], nice=False, clamp=True)
    chart = (
        alt.Chart(df_avg)
          .mark_line(point=True)
          .encode(
              x=alt.X("R:Q", title="R"),
              y=alt.Y("avg_y:Q", title=title_y, scale=y_scale),
              tooltip=[
                  alt.Tooltip("R:Q", title="R"),
                  alt.Tooltip("avg_y:Q", title=title_y, format=".2f"),
                  alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
              ],
          )
          .properties(height=340)
    )
    st.altair_chart(chart, use_container_width=True, theme=None)

with colF:
    st.markdown("**FR2 ‚Äî Average F by R**")
    _make_avg_chart(aggF, "Average F")

with colG2:
    st.markdown("**FR2 ‚Äî Average G by R**")
    _make_avg_chart(aggG_2, "Average G")

with colH:
    st.markdown("**FR2 ‚Äî Average H by R**")
    _make_avg_chart(aggH, "Average H")

# ---------- FR2: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ F / G / H (–ø–æ —Ç–∏–ø—É "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π") ----------
st.markdown("---")
st.subheader(f"Form Responses 2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ F / G / H (–≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å: {granularity.lower()})")

def _prep_df2_numeric_dist(df_src: pd.DataFrame, value_col: str, granularity: str):
    """–ì–æ—Ç–æ–≤–∏–º df —Å bucket‚Äô–∞–º–∏ –∏ —Å–ø–∏—Å–∫–æ–º –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–∏–Ω—Ç)."""
    if df_src.empty or value_col not in df_src.columns:
        return pd.DataFrame(), [], [], value_col
    d = df_src.copy()
    d = d.dropna(subset=["A", value_col])
    # –ø—Ä–∏–≤–æ–¥–∏–º –∫ —á–∏—Å–ª—É –∏ —É–±–∏—Ä–∞–µ–º NaN
    d[value_col] = pd.to_numeric(d[value_col], errors="coerce")
    d = d.dropna(subset=[value_col])
    if d.empty:
        return pd.DataFrame(), [], [], value_col

    # —Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —à–∫–∞–ª—ã
    d[value_col] = d[value_col].astype(int)

    # –¥–æ–±–∞–≤–ª—è–µ–º buckets –∏ –ø–æ–¥–ø–∏—Å–∏
    d = add_bucket(d, "A", granularity)
    d = ensure_bucket_and_label(d, "A", granularity)

    # –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ü–µ–ª—ã–µ)
    allowed_values = sorted(d[value_col].dropna().unique().tolist())
    if not allowed_values:
        return pd.DataFrame(), [], [], value_col

    # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —É—Ç–∏–ª–∏—Ç—É, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å out/pct –∏ –ø–æ—Ä—è–¥–∫–∏
    out, bucket_order, val_order, title = prep_distribution(d, value_col, allowed_values, value_col)
    return out, bucket_order, val_order, title

# –∏—Å—Ç–æ—á–Ω–∏–∫ —Å —É—á—ë—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —É—Ä–æ–∫–∞–º (R)
df2_numeric_src = _apply_r_filter(df2_base)

cF, cG, cH = st.columns(3)

with cF:
    st.markdown("**FR2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ F**")
    outF, orderF, valsF, titleF = _prep_df2_numeric_dist(df2_numeric_src, "F", granularity)
    if outF.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–ª–æ–Ω–∫–µ F –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        barsF = (
            alt.Chart(outF).mark_bar(size=BAR_SIZE.get(granularity, 36))
              .encode(
                  x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=orderF),
                  y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                  color=alt.Color("val_str:N", title=titleF, sort=valsF),
                  order=alt.Order("val:Q", sort="ascending"),
                  tooltip=[
                      alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                      alt.Tooltip("val_str:N", title=titleF),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%"),
                  ],
              )
              .properties(height=420)
        )
        st.altair_chart(barsF, use_container_width=True, theme=None)

with cG:
    st.markdown("**FR2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ G**")
    outG, orderG, valsG, titleG = _prep_df2_numeric_dist(df2_numeric_src, "G", granularity)
    if outG.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–ª–æ–Ω–∫–µ G –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        barsG = (
            alt.Chart(outG).mark_bar(size=BAR_SIZE.get(granularity, 36))
              .encode(
                  x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=orderG),
                  y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                  color=alt.Color("val_str:N", title=titleG, sort=valsG),
                  order=alt.Order("val:Q", sort="ascending"),
                  tooltip=[
                      alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                      alt.Tooltip("val_str:N", title=titleG),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%"),
                  ],
              )
              .properties(height=420)
        )
        st.altair_chart(barsG, use_container_width=True, theme=None)

with cH:
    st.markdown("**FR2 ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ H**")
    outH, orderH, valsH, titleH = _prep_df2_numeric_dist(df2_numeric_src, "H", granularity)
    if outH.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–ª–æ–Ω–∫–µ H –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        barsH = (
            alt.Chart(outH).mark_bar(size=BAR_SIZE.get(granularity, 36))
              .encode(
                  x=alt.X("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥", sort=orderH),
                  y=alt.Y("sum(count):Q", title="–ö–æ–ª-–≤–æ –æ—Ç–≤–µ—Ç–æ–≤"),
                  color=alt.Color("val_str:N", title=titleH, sort=valsH),
                  order=alt.Order("val:Q", sort="ascending"),
                  tooltip=[
                      alt.Tooltip("bucket_label:N", title="–ü–µ—Ä–∏–æ–¥"),
                      alt.Tooltip("val_str:N", title=titleH),
                      alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                      alt.Tooltip("pct:Q", title="% –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–∏–æ–¥–∞", format=".0%"),
                  ],
              )
              .properties(height=420)
        )
        st.altair_chart(barsH, use_container_width=True, theme=None)

# ---------- FR2: –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî R) ‚Äî –≥—Ä–∞—Ñ–∏–∫–∏ (–≤ %) –¥–ª—è F / G / H ----------
st.markdown("---")
st.subheader("Form Responses 2 ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (–æ—Å—å X ‚Äî R) ‚Äî –≥—Ä–∞—Ñ–∏–∫–∏ (–≤ %)")

def _build_numeric_counts_by_R(df_src: pd.DataFrame, value_col: str) -> pd.DataFrame:
    """
    –°—á–∏—Ç–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–∞–∂–¥–æ–µ —Ü–µ–ª–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —à–∫–∞–ª—ã –≤ –∫–æ–ª–æ–Ω–∫–µ value_col
    –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–∫–∞ R. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏: R, val, val_str, count, total.
    """
    if df_src.empty or value_col not in df_src.columns or "R" not in df_src.columns:
        return pd.DataFrame(columns=["R","val","val_str","count","total"])

    d = df_src.copy()
    d["R"] = pd.to_numeric(d["R"], errors="coerce")
    d[value_col] = pd.to_numeric(d[value_col], errors="coerce")
    d = d.dropna(subset=["R", value_col])
    if d.empty:
        return pd.DataFrame(columns=["R","val","val_str","count","total"])

    d["R"] = d["R"].astype(int)
    d["val"] = d[value_col].astype(int)
    d["val_str"] = d["val"].astype(str)

    grp = (d.groupby(["R","val","val_str"], as_index=False)
             .size().rename(columns={"size":"count"}))
    totals = (grp.groupby("R", as_index=False)["count"]
                .sum().rename(columns={"count":"total"}))
    out = grp.merge(totals, on="R", how="left")
    return out

def _make_percent_stack_by_R(out_df: pd.DataFrame, legend_title: str):
    """
    –†–∏—Å—É–µ—Ç –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–µ–∫ –ø–æ R (0‚Äì100%) —Å —Ç—É–ª—Ç–∏–ø–æ–º: —É—Ä–æ–∫, –∑–Ω–∞—á–µ–Ω–∏–µ, count, pct, total.
    """
    if out_df.empty:
        return None

    # –ø–æ—Ä—è–¥–æ–∫ —É—Ä–æ–∫–æ–≤ –∏ –∑–Ω–∞—á–µ–Ω–∏–π
    r_order = sorted(out_df["R"].unique().tolist())
    val_order = sorted(out_df["val"].unique().tolist())
    val_order_str = [str(v) for v in val_order]

    base = (
        alt.Chart(out_df)
          .transform_calculate(pct='datum.count / datum.total')
    )

    chart = (
        base.mark_bar(size=28, stroke=None, strokeWidth=0)
            .encode(
                x=alt.X("R:O", title="R", sort=r_order),
                y=alt.Y(
                    "count:Q",
                    stack="normalize",
                    axis=alt.Axis(format="%", title="% –æ—Ç –æ—Ç–≤–µ—Ç–æ–≤"),
                    scale=alt.Scale(domain=[0, 1], nice=False, clamp=True)
                ),
                color=alt.Color(
                    "val_str:N",
                    title=legend_title,
                    sort=val_order_str,
                    legend=alt.Legend(
                        orient="bottom",
                        direction="horizontal",
                        columns=3,
                        labelLimit=1000,
                        titleLimit=1000,
                        symbolType="square",
                    ),
                ),
                tooltip=[
                    alt.Tooltip("R:O", title="–£—Ä–æ–∫ (R)"),
                    alt.Tooltip("val_str:N", title=legend_title),
                    alt.Tooltip("count:Q", title="–ö–æ–ª-–≤–æ"),
                    alt.Tooltip("pct:Q", title="–î–æ–ª—è", format=".0%"),
                    alt.Tooltip("total:Q", title="–í—Å–µ–≥–æ –ø–æ —É—Ä–æ–∫—É"),
                ],
            )
    ).configure_legend(labelLimit=1000, titleLimit=1000)

    return chart

# –∏—Å—Ç–æ—á–Ω–∏–∫ —Å —É—á—ë—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —É—Ä–æ–∫–∞–º (R)
df2_lessons_src = _apply_r_filter(df2_base)

cF2, cG2, cH2 = st.columns(3)

with cF2:
    st.markdown("**FR2 ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (R) ‚Äî F (–≤ %)**")
    outF_R = _build_numeric_counts_by_R(df2_lessons_src, "F")
    if outF_R.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ F –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        chF_R = _make_percent_stack_by_R(outF_R, "F")
        st.altair_chart(chF_R.properties(height=460), use_container_width=True, theme=None)

with cG2:
    st.markdown("**FR2 ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (R) ‚Äî G (–≤ %)**")
    outG_R = _build_numeric_counts_by_R(df2_lessons_src, "G")
    if outG_R.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ G –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        chG_R = _make_percent_stack_by_R(outG_R, "G")
        st.altair_chart(chG_R.properties(height=460), use_container_width=True, theme=None)

with cH2:
    st.markdown("**FR2 ‚Äî –ø–æ —É—Ä–æ–∫–∞–º (R) ‚Äî H (–≤ %)**")
    outH_R = _build_numeric_counts_by_R(df2_lessons_src, "H")
    if outH_R.empty:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ H –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤.")
    else:
        chH_R = _make_percent_stack_by_R(outH_R, "H")
        st.altair_chart(chH_R.properties(height=460), use_container_width=True, theme=None)

